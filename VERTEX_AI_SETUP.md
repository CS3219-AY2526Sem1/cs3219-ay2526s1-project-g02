# Vertex AI Setup Complete! ðŸ¤–

## âœ… What Was Done

### 1. **Enabled Vertex AI API**
- âœ… `aiplatform.googleapis.com` enabled in project `noclue-476404`

### 2. **Granted Permissions**
- âœ… Service account: `pubsub-service@noclue-476404.iam.gserviceaccount.com`
- âœ… Role: `roles/aiplatform.user`
- âœ… Can now access Gemini models

### 3. **Configured LLM Service**
- âœ… Added `@google-cloud/vertexai` package
- âœ… Created `LlmVertexService` (uses Gemini)
- âœ… Updated `LlmController` to support both providers
- âœ… Set `LLM_PROVIDER=vertex` in K8s deployment

---

## ðŸŽ¯ How It Works

### Architecture:

```
LLM Service Pod
    â†“
Reads: LLM_PROVIDER=vertex (env var)
    â†“
Uses: LlmVertexService (not LlmService)
    â†“
Authenticates with: /etc/gcp/key.json (pubsub-key secret)
    â†“
Calls: Vertex AI API
    â†“
Model: gemini-1.5-flash
```

### Configuration:

```yaml
# LLM Service Environment
LLM_PROVIDER: "vertex"              # Use Vertex AI
GCP_PROJECT_ID: "noclue-476404"     # Your GCP project
GCP_LOCATION: "us-central1"         # Vertex AI region
GCP_KEY_FILENAME: "/etc/gcp/key.json" # Service account key
```

**No `OPENAI_API_KEY` needed!** âœ…

---

## ðŸ’° Cost Savings

### Vertex AI Gemini Pricing (Free Tier):

| Model | Free Tier | After Free Tier |
|-------|-----------|-----------------|
| **Gemini 1.5 Flash** | **1500 req/day** | $0.075 per 1M chars |
| Gemini 1.5 Pro | 50 req/day | $1.25 per 1M chars |
| Gemini 1.0 Pro | 60 req/min | $0.50 per 1M chars |

**Your app uses:** `gemini-1.5-flash` âœ…

### Comparison:

**OpenAI (gpt-4o-mini):**
- Cost: $0.15 per 1M input tokens
- No free tier
- Requires separate API key management

**Vertex AI (gemini-1.5-flash):**
- âœ… **FREE: 1500 requests/day**
- âœ… Uses GCP credits after free tier
- âœ… Same authentication as other GCP services
- âœ… Integrated billing

**Savings:** If you make <1500 requests/day, it's **100% FREE!** ðŸŽ‰

---

## ðŸ§ª Testing Vertex AI

### 1. Check LLM Service Logs

```bash
kubectl logs -f deployment/llm-service -n noclue-app
```

Look for:
```
Vertex AI initialized with project: noclue-476404, location: us-central1
```

### 2. Test Question Explanation

```bash
# Get LLM service IP
LLM_IP=$(kubectl get service llm-service -n noclue-app -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# Test the endpoint
curl -X POST http://${LLM_IP}:4005/llm/explain-question \
  -H "Content-Type: application/json" \
  -d '{
    "questionId": "YOUR_QUESTION_ID"
  }'
```

### 3. Monitor Usage

Check Vertex AI usage in GCP Console:
```
https://console.cloud.google.com/vertex-ai/generative/dashboard?project=noclue-476404
```

---

## ðŸ”„ Switching Between Providers

You can switch between OpenAI and Vertex AI anytime:

### Use Vertex AI (Free Tier):
```yaml
# k8s/llm-service-deployment.yaml
env:
  - name: LLM_PROVIDER
    value: "vertex"  # â† Uses Gemini
```

### Use OpenAI:
```yaml
env:
  - name: LLM_PROVIDER
    value: "openai"  # â† Uses GPT-4o-mini
  - name: OPENAI_API_KEY
    valueFrom:
      secretKeyRef:
        name: supabase-secrets
        key: OPENAI_API_KEY  # â† Need to add this secret
```

---

## ðŸ“‹ Required Permissions Summary

For Vertex AI to work, you need:

1. âœ… **API Enabled:** `aiplatform.googleapis.com`
2. âœ… **Service Account:** `pubsub-service@noclue-476404.iam.gserviceaccount.com`
3. âœ… **IAM Role:** `roles/aiplatform.user`
4. âœ… **Credentials:** Mounted via `pubsub-key` secret
5. âœ… **Environment Vars:** `GCP_PROJECT_ID`, `GCP_KEY_FILENAME`, `LLM_PROVIDER`

**All done!** âœ…

---

## ðŸš¨ After Code Changes: Rebuild Required

Since I updated the code (`LlmController.ts`), you need to **rebuild the Docker image** for changes to take effect:

### Option 1: Push to Main (Triggers CI/CD)
```bash
git push origin HEAD
# Or merge to main and push
```

### Option 2: Build Manually
```bash
# Build new image
docker build -f Dockerfile.llm-service -t gcr.io/noclue-476404/llm-service:latest .

# Push to registry
docker push gcr.io/noclue-476404/llm-service:latest

# Restart deployment
kubectl rollout restart deployment/llm-service -n noclue-app
```

---

## âœ… Summary

**Vertex AI is ready to use!**

âœ… **Permissions:** Granted
âœ… **API:** Enabled
âœ… **Code:** Updated to support Vertex AI
âœ… **Deployment:** Configured with LLM_PROVIDER=vertex
âœ… **Cost:** FREE (1500 requests/day)

**What you need:**
1. Push/merge to main to rebuild LLM service Docker image
2. That's it! No API keys needed.

**Benefits:**
- ðŸ’° FREE tier (1500 requests/day)
- ðŸš€ Fast (Gemini 1.5 Flash is optimized for speed)
- ðŸ” Same auth as Pub/Sub (no extra config)
- ðŸ’³ Uses your GCP credits

---

## ðŸŽ“ Permissions Breakdown

The `pubsub-service` service account now has these roles:

| Role | Purpose |
|------|---------|
| `roles/pubsub.publisher` | Publish messages to Pub/Sub |
| `roles/pubsub.subscriber` | Subscribe to Pub/Sub topics |
| `roles/pubsub.viewer` | View Pub/Sub resources |
| `roles/aiplatform.user` | **Use Vertex AI / Gemini models** |

All accessed via the same key file: `/etc/gcp/key.json` (from `pubsub-key` secret).

**No additional setup needed!** The LLM service will use Vertex AI automatically. ðŸŽ‰

